{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skoleti123/Python_AI/blob/main/Sklearn_Demo_AIET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Learning Notebook: SkLearn"
      ],
      "id": "Ps9llghv8jX1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing with scikit-learn\n",
        "\n",
        "### Introduction\n",
        "In this notebook, you will apply various preprocessing techniques with scikit-learn (`sklearn`) to the Titanic dataset in order to prepare the data for predictive modeling.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will be able to:\n",
        "\n",
        "* Understand the SkLearn Machine Learning framework\n",
        "* Practice applying `sklearn.impute` to fill in missing values\n",
        "* Practice applying `sklearn.preprocessing`:\n",
        "  * Scale the features using `StandardScaler` and `MinMaxScaler`\n",
        "  * `LabelEncoder` for converting categories to 0 and 1 within a single column\n",
        "  * `OneHotEncoder` for creating multiple \"dummy\" columns to represent multiple categories\n",
        "* Split the data into train and test for model training and evaluation.\n",
        "* Loading Datasets from scikit-learn for various ML practices."
      ],
      "metadata": {
        "id": "kfVjAbJnfduh"
      },
      "id": "kfVjAbJnfduh"
    },
    {
      "cell_type": "markdown",
      "id": "5d352c24",
      "metadata": {
        "id": "5d352c24"
      },
      "source": [
        "### Titanic Dataset\n",
        "\n",
        "In this assignment, we will preprocess the Titanic dataset to practice data cleaning, encoding, scaling, binarization, polynomial features, discretization, and feature selection. We will use Scikit-learn's preprocessing functions to make the data ready for machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step1: Data Loading"
      ],
      "metadata": {
        "id": "Kr2tohoOgxbI"
      },
      "id": "Kr2tohoOgxbI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0ca260",
      "metadata": {
        "id": "ff0ca260"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d57c53",
      "metadata": {
        "id": "43d57c53"
      },
      "source": [
        "### Step 2: Handling Missing Values\n",
        "\n",
        "**Imputation of missing values**\n",
        "\n",
        "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n",
        "\n",
        "The Imputer class provides basic strategies for imputing missing values, either using the mean, the median or the most frequent value of the row or column in which the missing values are located. This class also allows for different missing values encodings.\n",
        "\n",
        "[SimpleImputer](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create a sample DataFrame with missing values\n",
        "example0 = {'Age': [25, 30, np.nan, 45, 50],\n",
        "        'Salary': [50000, 60000, 70000, np.nan, 90000]}\n",
        "df_0 = pd.DataFrame(example0)\n",
        "# Create a SimpleImputer object with a specific strategy\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "# Fit the imputer to the data and transform it\n",
        "df_imputed = imputer.fit_transform(df_0)\n",
        "# Convert the imputed data back to a DataFrame\n",
        "df_imputed = pd.DataFrame(df_imputed, columns=df_0.columns)\n",
        "print(df_imputed)"
      ],
      "metadata": {
        "id": "ymhPxFmBTM3q"
      },
      "id": "ymhPxFmBTM3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the missing values\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "ghhCoESyqZ_p"
      },
      "id": "ghhCoESyqZ_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab8ba35",
      "metadata": {
        "id": "bab8ba35"
      },
      "outputs": [],
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.impute import SimpleImputer\n",
        "# Impute numerical columns\n",
        "imputer_num = SimpleImputer(strategy=\"median\")\n",
        "data[\"Age\"] = imputer_num.fit_transform(data[[\"Age\"]])\n",
        "\n",
        "# Impute categorical columns\n",
        "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
        "data[\"Embarked\"] = imputer_cat.fit_transform(data[[\"Embarked\"]]).ravel()\n",
        "\n",
        "# Impute string column\n",
        "imputer_str = SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")\n",
        "data[\"Cabin\"] = imputer_str.fit_transform(data[[\"Cabin\"]]).ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea84d64",
      "metadata": {
        "id": "bea84d64"
      },
      "source": [
        "### Step 3: Encoding Categorical Variables\n",
        "\n",
        "#### LabelEncoder\n",
        "LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. It's a simple approach where each unique category is assigned a unique integer value. This transformation is necessary because most machine learning algorithms require numerical input.\n",
        "\n",
        "[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
        "\n",
        "#### OneHotEncoder\n",
        "\n",
        " It involves creating new binary columns for each category, where a \"1\" indicates the presence of that category and a \"0\" indicates its absence.\n",
        "\n",
        "For example, consider a dataset with a column for \"Color\" containing the categories \"Red,\" \"Green,\" and \"Blue.\" One-hot encoding would transform this column into three new columns: \"Color_Red,\" \"Color_Green,\" and \"Color_Blue.\" For a row with the value \"Red,\" the resulting values in the new columns would be [1, 0, 0].\n",
        "\n",
        "[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f5e9834",
      "metadata": {
        "id": "4f5e9834"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label Encoding for binary categorical column 'Sex'\n",
        "le = LabelEncoder()\n",
        "data[\"Sex\"] = le.fit_transform(data[\"Sex\"])\n",
        "print(\"\\nAfter label encoding 'Sex':\\n\", data[\"Sex\"].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# One-hot encoding for multi-class categorical column 'Embarked'\n",
        "ohe = OneHotEncoder()\n",
        "encoded_embarked = ohe.fit_transform(data[[\"Embarked\"]])\n",
        "encoded_embarked_df = pd.DataFrame(encoded_embarked.toarray(), columns=ohe.get_feature_names_out([\"Embarked\"]))\n",
        "data = pd.concat([data.drop('Embarked',axis=1), encoded_embarked_df], axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "sBfZeJFehvr3"
      },
      "id": "sBfZeJFehvr3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "22f071fb",
      "metadata": {
        "id": "22f071fb"
      },
      "source": [
        "### Step 4: Feature Scaling\n",
        "\n",
        "Feature scaling is crucial in machine learning to ensure that all features contribute equally to the model's predictions. It improves model performance, fairness, and visualization.\n",
        "\n",
        "#### MinMaxScaler:\n",
        "\n",
        "Scales features to a specific range (e.g., 0 to 1).\n",
        "Suitable for algorithms sensitive to feature scales.\n",
        "\n",
        "  The transformation is given by:\n",
        "\n",
        "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "\n",
        "    X_scaled = X_std * (max - min) + min\n",
        "\n",
        "[MinMaxScaler](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
        "\n",
        "#### StandardScaler:\n",
        "\n",
        "Standardizes features to have zero mean and unit variance.\n",
        "Suitable for algorithms assuming normal distribution (e.g., Linear Regression, Logistic Regression).\n",
        "Less sensitive to outliers than MinMaxScaler.\n",
        "\n",
        "[StandardScaler](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "example1 = {'feature1': [1, 2, 3, 4, 5],\n",
        "        'feature2': [10, 20, 30, 40, 50]}\n",
        "df_1 = pd.DataFrame(example1)\n",
        "\n",
        "# MinMaxScaler example\n",
        "scaler = MinMaxScaler()\n",
        "df_minmax = scaler.fit_transform(df_1)\n",
        "print(\"MinMaxScaler output:\\n\", df_minmax)\n",
        "\n",
        "# StandardScaler example\n",
        "scaler = StandardScaler()\n",
        "df_standard = scaler.fit_transform(df_1)\n",
        "print(\"\\nStandardScaler output:\\n\", df_standard)\n"
      ],
      "metadata": {
        "id": "dn21QJ72s5Hc"
      },
      "id": "dn21QJ72s5Hc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db1ae1d",
      "metadata": {
        "id": "0db1ae1d"
      },
      "outputs": [],
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Standard Scaling for 'Fare'\n",
        "scaler_standard = StandardScaler()\n",
        "data[\"Fare\"] = scaler_standard.fit_transform(data[[\"Fare\"]])\n",
        "print(\"\\nAfter standard scaling 'Fare':\\n\", data[\"Fare\"].head())\n",
        "\n",
        "# Min-Max Scaling for 'Age'\n",
        "scaler_minmax = MinMaxScaler()\n",
        "data[\"Age\"] = scaler_minmax.fit_transform(data[[\"Age\"]])\n",
        "print(\"\\nAfter min-max scaling 'Age':\\n\", data[\"Age\"].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf826b3",
      "metadata": {
        "id": "3bf826b3"
      },
      "source": [
        "### Step 5: Binarization\n",
        "\n",
        "Binarization is a technique used to convert continuous numerical features into binary (0 or 1) categorical features. This is often done to simplify the model or to create new features that capture specific information.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Suppose we have a feature \"Age\". We can binarize it to create a new feature \"Is_Adult\" where:\n",
        "\n",
        "  * If age >= 18, \"Is_Adult\" = 1 (Adult)\n",
        "  * If age < 18, \"Is_Adult\" = 0 (Child)\n",
        "\n",
        "[Binarizer](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.Binarizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# Sample data\n",
        "example2 = {'Age': [15, 25, 35, 45, 55]}\n",
        "df_2 = pd.DataFrame(example2)\n",
        "\n",
        "# Create a Binarizer object with a threshold of 18\n",
        "binarizer = Binarizer(threshold=18)\n",
        "\n",
        "# Transform the 'Age' column\n",
        "df_2['Is_Adult'] = binarizer.fit_transform(df_2[['Age']])\n",
        "\n",
        "print(df_2)"
      ],
      "metadata": {
        "id": "lYUQ2jiLtv-B"
      },
      "id": "lYUQ2jiLtv-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd73fc23",
      "metadata": {
        "id": "bd73fc23"
      },
      "outputs": [],
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "binarizer = Binarizer(threshold=25)\n",
        "data[\"Age_binarized\"] = binarizer.fit_transform(data[[\"Age\"]])\n",
        "print(\"\\nAfter binarizing 'Age':\\n\", data[[\"Age\", \"Age_binarized\"]].head())\n",
        "data[\"Age_binarized\"].unique(), data[\"Age\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645eebd5",
      "metadata": {
        "id": "645eebd5"
      },
      "source": [
        "### Step 6: Discretization\n",
        "\n",
        "Discretization is a technique used to convert continuous numerical features into discrete or categorical features. This can be helpful for various reasons, such as:\n",
        "\n",
        "* Improving the performance of certain algorithms\n",
        "* Reducing the impact of noise and outliers\n",
        "* Creating more interpretable models\n",
        "\n",
        "KBinsDiscretizer is a class in scikit-learn that implements binning strategies for numerical features. It divides the range of a feature into a specified number of bins.\n",
        "\n",
        "[KBinsDiscretizer](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "# Sample data\n",
        "example3 = {'Age': [18, 25, 30, 35, 40, 45, 50, 55, 60]}\n",
        "df_3 = pd.DataFrame(example3)\n",
        "\n",
        "# Create a KBinsDiscretizer object with 3 bins\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
        "# Fit the discretizer to the data\n",
        "discretizer.fit(df_3[['Age']])\n",
        "# Transform the data\n",
        "df_3['Age_Discretized'] = discretizer.transform(df_3[['Age']])\n",
        "df_3"
      ],
      "metadata": {
        "id": "MTa-yEDbueZ6"
      },
      "id": "MTa-yEDbueZ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67631b6c",
      "metadata": {
        "id": "67631b6c"
      },
      "outputs": [],
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\")\n",
        "data[\"Fare_binned\"] = discretizer.fit_transform(data[[\"Fare\"]])\n",
        "print(\"\\nAfter discretizing 'Fare':\\n\", data[[\"Fare\", \"Fare_binned\"]].head())\n",
        "data[\"Fare_binned\"].unique(), data[\"Fare\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db96a492",
      "metadata": {
        "id": "db96a492"
      },
      "source": [
        "### Step 7: Polynomial Features\n",
        "\n",
        "\n",
        "Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in PolynomialFeatures:\n",
        "\n",
        "[PolynomialFeatures](https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "r12oi3XXfLr8"
      },
      "outputs": [],
      "source": [
        "######## EXAMPLE #########\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "X = np.arange(6).reshape(3, 2)\n",
        "poly = PolynomialFeatures(2)\n",
        "poly.fit_transform(X)"
      ],
      "id": "r12oi3XXfLr8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3b4cc4",
      "metadata": {
        "id": "ab3b4cc4"
      },
      "outputs": [],
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "poly_features = poly.fit_transform(data[[\"Age\", \"Fare\"]])\n",
        "poly_feature_names = poly.get_feature_names_out([\"Age\", \"Fare\"])\n",
        "data[poly_feature_names] = poly_features\n",
        "print(\"\\nAfter generating polynomial features:\\n\", data[poly_feature_names].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b01e6440"
      },
      "source": [
        "### Step 8: Power Transformation\n",
        "\n",
        "Power transformation is a technique used to transform numerical features to make their distribution more Gaussian-like. This is particularly useful for machine learning algorithms that assume normality, such as linear regression and Gaussian Naive Bayes.\n",
        "\n",
        "Why Power Transform?\n",
        "\n",
        "* Improved Model Performance: Many machine learning models perform better when the features are normally distributed.\n",
        "* Stabilized Variance: Power transformations can help stabilize the variance of features.\n",
        "* Reduced Skewness: Skewed distributions can be transformed into more symmetric distributions.\n",
        "\n",
        "Common Power Transformations:\n",
        "\n",
        "1. Box-Cox Transformation:\n",
        "  * A versatile transformation that can handle both positive and negative values.\n",
        "  * It involves raising each data point to a power, often determined using maximum likelihood estimation.\n",
        "2. Yeo-Johnson Transformation:\n",
        "  * A variation of the Box-Cox transformation that can handle both positive and negative values, including zero.\n",
        "  * It's more robust to outliers and can be used in cases where the Box-Cox transformation is not suitable.\n",
        "Implementation in scikit-learn:\n",
        "\n",
        "[PowerTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)"
      ],
      "id": "b01e6440"
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Sample data\n",
        "example4 = {'feature1': [1, 2, 3, 4, 5],\n",
        "        'feature2': [10, 20, 30, 40, 50]}\n",
        "df_4 = pd.DataFrame(example4)\n",
        "# Create a PowerTransformer object\n",
        "transformer = PowerTransformer(method='yeo-johnson')\n",
        "# Fit and transform the data\n",
        "df_transformed = transformer.fit_transform(df_4)\n",
        "df_transformed"
      ],
      "metadata": {
        "id": "6B6svxVDv-t3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6B6svxVDv-t3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27b4a8bd"
      },
      "outputs": [],
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "power_transformer = PowerTransformer()\n",
        "data[\"Fare_power\"] = power_transformer.fit_transform(data[[\"Fare\"]])\n",
        "print(\"\\nAfter power transforming 'Fare':\\n\", data[[\"Fare\", \"Fare_power\"]].head())"
      ],
      "id": "27b4a8bd"
    },
    {
      "cell_type": "markdown",
      "id": "a433bfed",
      "metadata": {
        "id": "a433bfed"
      },
      "source": [
        "### Step 9: Feature Selection\n",
        "\n",
        "Feature selection helps identify the most relevant features, reducing dimensionality and minimizing noise.\n",
        "\n",
        "Using SelectKBest to choose the top features based on the ANOVA F-value selects features most related to the target variable (Survived), which reduces the dataset size and potentially improves model efficiency.\n",
        "\n",
        "Feature selection can lead to simpler, faster, and sometimes more accurate models by focusing on only the most predictive features, thereby improving model generalization and reducing overfitting.\n",
        "\n",
        "[SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
        "\n",
        "[f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c20e9c",
      "metadata": {
        "id": "01c20e9c"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Dropping non-numeric columns\n",
        "data_numeric = data.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
        "\n",
        "# Define features and target\n",
        "X = data_numeric.drop(\"Survived\", axis=1)\n",
        "y = data_numeric[\"Survived\"]\n",
        "\n",
        "# Apply SelectKBest\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support(indices=True)]\n",
        "print(\"\\nSelected features based on ANOVA F-value:\", selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e2944b",
      "metadata": {
        "id": "19e2944b"
      },
      "source": [
        "### Step 10: Splitting the Data\n",
        "\n",
        "In machine learning, it's essential to evaluate a model's performance on unseen data to assess its generalization ability. The train-test split technique is a common approach to achieve this.\n",
        "\n",
        "**Model training and testing**\n",
        "\n",
        "![wget](https://cdn.iisc.talentsprint.com/CDS/Images/model_train_test1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWDdGhE4miC9"
      },
      "source": [
        "#### Training, Validation, and Test Set"
      ],
      "id": "uWDdGhE4miC9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHbUne3bm5O5"
      },
      "source": [
        "A machine learning algorithm splits the Dataset into two sets.\n",
        "\n",
        "Splitting your dataset is essential for an unbiased evaluation of prediction performance. In most cases, it’s enough to split your dataset randomly into two subsets:\n",
        "\n",
        "**Training Dataset:** The sample of data used to fit the model.\n",
        "\n",
        "**Test Dataset:** The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
        "\n",
        "We usually split the data as 80% for training stage and 20% for testing stage. 70% train and 30% test or 75% train and 25% test are also often used.\n",
        "\n",
        "**Validation Set:** This is a separate section of your dataset that you will use during training to get a sense of how well your model is doing on data that are not being used in training.\n",
        "\n",
        "In less complex cases, when you don’t have to tune hyperparameters, it’s okay to work with only the training and test sets.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*aNPC1ifHN2WydKHyEZYENg.png\" alt=\"drawing\" width=\"500\"/>\n"
      ],
      "id": "QHbUne3bm5O5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4190b8dd",
      "metadata": {
        "id": "4190b8dd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Using the selected features\n",
        "X = data[selected_features]  # Use only selected features for training\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Splitting into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"\\nFirst 5 rows of training set:\\n\", X_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2225fba5",
      "metadata": {
        "id": "2225fba5"
      },
      "source": [
        "### Summary\n",
        "\n",
        "In this assignment, you preprocessed the Titanic dataset by handling missing values, encoding categorical variables, scaling features, creating interaction terms with polynomial features, binarizing, and finally selecting the top features for the model input.\n",
        "\n",
        "This preparation is essential for ensuring that the data is well-suited for machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "H9SXUlPYfLpx"
      },
      "source": [
        "## Datasets from scikit-learn\n",
        "\n",
        "There are three distinct kinds of dataset interfaces for different types of datasets. The simplest one is the interface for sample images, which is described below in the Sample images section.\n",
        "\n",
        "The dataset generation functions and the svmlight loader share a simplistic interface, returning a tuple `(X, y)` consisting of a `n_samples * n_features` numpy array `X` and an array of length n_samples containing the targets `y`.\n",
        "\n",
        "The toy datasets as well as the ‘real world’ datasets and the datasets fetched from mldata.org have more sophisticated structure. These functions return a dictionary-like object holding at least two items: an array of shape `n_samples * n_features` with key `data` (except for 20newsgroups) and a numpy array of length `n_samples`, containing the target values, with key `target`.\n",
        "\n",
        "The datasets also contain a description in `DESCR` and some contain `feature_names` and `target_names`. See the dataset descriptions below for details."
      ],
      "id": "H9SXUlPYfLpx"
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "import sklearn.datasets as datasets"
      ],
      "metadata": {
        "id": "FTj6NSfr1itT"
      },
      "id": "FTj6NSfr1itT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "HYoELmX7fLp7"
      },
      "source": [
        "### Sample Images\n",
        "\n",
        "The scikit also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those image can be useful to test algorithms and pipeline on 2D data."
      ],
      "id": "HYoELmX7fLp7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "lu5B3zKQfLp9"
      },
      "outputs": [],
      "source": [
        "# datasets.load_sample_images()\n",
        "china = datasets.load_sample_image('china.jpg')\n",
        "\n",
        "flower = datasets.load_sample_image('flower.jpg')"
      ],
      "id": "lu5B3zKQfLp9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "XBZ4UPYKfLp-"
      },
      "outputs": [],
      "source": [
        "plt.imshow(china)"
      ],
      "id": "XBZ4UPYKfLp-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5pN2qBWtfLp_"
      },
      "outputs": [],
      "source": [
        "plt.imshow(flower)"
      ],
      "id": "5pN2qBWtfLp_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-h76Fv_fLqB"
      },
      "outputs": [],
      "source": [
        "flower.shape"
      ],
      "id": "H-h76Fv_fLqB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Jk4quvZ7fLqD"
      },
      "source": [
        "### Sample Generators\n",
        "\n",
        "In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\n",
        "\n",
        "All of the generators are prefixed with the word `make`"
      ],
      "id": "Jk4quvZ7fLqD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6Q9R_6eCfLqE"
      },
      "outputs": [],
      "source": [
        "datasets.make_blobs?"
      ],
      "id": "6Q9R_6eCfLqE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dILoKEPvfLqG"
      },
      "outputs": [],
      "source": [
        "X, y = datasets.make_blobs()"
      ],
      "id": "dILoKEPvfLqG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QYL7bTyDfLqH"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ],
      "id": "QYL7bTyDfLqH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9pbkwKg1fLqI"
      },
      "source": [
        "### Toy and Fetched Datasets\n",
        "\n",
        "Scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\n",
        "\n",
        "These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in the scikit. They are however often too small to be representative of real world machine learning tasks.\n",
        "\n",
        "These datasets are prefixed with the `load` command."
      ],
      "id": "9pbkwKg1fLqI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Gl1vcytmfLqI"
      },
      "outputs": [],
      "source": [
        "housing = datasets.fetch_california_housing()"
      ],
      "id": "Gl1vcytmfLqI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "0cSU3HITfLqJ"
      },
      "outputs": [],
      "source": [
        "housing.keys()"
      ],
      "id": "0cSU3HITfLqJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "sGO9j273fLqK"
      },
      "outputs": [],
      "source": [
        "housing.data.shape, housing.target.shape"
      ],
      "id": "sGO9j273fLqK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Y6_jBAwifLqL"
      },
      "outputs": [],
      "source": [
        "housing.feature_names"
      ],
      "id": "Y6_jBAwifLqL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xRpMTnOSfLqL"
      },
      "outputs": [],
      "source": [
        "print(housing.DESCR)"
      ],
      "id": "xRpMTnOSfLqL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZtQ4ueVUfLqM"
      },
      "source": [
        "#### Fetched datasets\n",
        "\n",
        "These are all somewhat unique with their own functions to fetch and load them. I'll go through a single one below. They are all prefixed with the word `fetch`"
      ],
      "id": "ZtQ4ueVUfLqM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "f1SkSJYKfLqN"
      },
      "outputs": [],
      "source": [
        "faces = datasets.fetch_olivetti_faces()"
      ],
      "id": "f1SkSJYKfLqN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DU1uQ2mHfLqN"
      },
      "outputs": [],
      "source": [
        "faces.keys()"
      ],
      "id": "DU1uQ2mHfLqN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "SrytAc9DfLqN"
      },
      "outputs": [],
      "source": [
        "faces.images.shape, faces.data.shape, faces.target.shape"
      ],
      "id": "SrytAc9DfLqN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "sklearn is a powerful Python library that provides various machine learning algorithms, including linear regression. We'll use its LinearRegression class and its fit and score functions to train and evaluate our model.\n",
        "\n",
        "[LinearRegression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ],
      "metadata": {
        "id": "5ldhRULoNV5L"
      },
      "id": "5ldhRULoNV5L"
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = datasets.fetch_california_housing(return_X_y=True) # Load the Boston Housing dataset\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LinearRegression() # Create a Linear Regression model\n",
        "model.fit(X_train, y_train) # Fit the model to the training data\n",
        "y_pred = model.predict(X_test) # Make predictions on the testing set\n",
        "mse = mean_squared_error(y_test, y_pred) # Evaluate the model's performance using Mean Squared Error\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "score = model.score(X_test, y_test)\n",
        "print(\"R-squared Score:\", score)"
      ],
      "metadata": {
        "id": "AKAP9D3FNaQT"
      },
      "id": "AKAP9D3FNaQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## APPLYING to DATA ########\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "X = data[selected_features]  # Use only selected features for training\n",
        "y = data[\"Survived\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression() # Create a Logistic Regression model\n",
        "model.fit(X_train, y_train) # Train the model\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "WnqboVFtPi48"
      },
      "id": "WnqboVFtPi48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Metrics\n",
        "\n",
        "Performance metrics are crucial for evaluating the effectiveness of a machine learning model. They help us understand how well our model performs on a given dataset. Two common metrics are `accuracy_score` and the `confusion matrix`.\n",
        "\n",
        "**Accuracy Score**\n",
        "\n",
        "The accuracy score is a simple metric that calculates the proportion of correct predictions made by the model. It's calculated as the ratio of correct predictions to total predictions.\n",
        "\n",
        "**Confusion Matrix**\n",
        "\n",
        "A confusion matrix is a table that summarizes the performance of a classification model on a set of test data. It provides a more detailed breakdown of correct and incorrect predictions.\n",
        "\n",
        "The confusion matrix would look like this:\n",
        "\n",
        "|      | Predicted Positive\t| Predicted Negative |\n",
        "|---|---|---|\n",
        "Actual Positive\t| True Positive (TP)\t|  False Negative (FN)\n",
        "Actual Negative\t| False Positive (FP)\t| True Negative (TN)\n",
        "\n",
        "**Classification Report:**\n",
        "\n",
        "A classification report is a performance evaluation metric for machine learning classification models. It provides a detailed breakdown of the model's performance, including precision, recall, F1-score, and support for each class.\n",
        "\n",
        "[classification_report](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html)\n"
      ],
      "metadata": {
        "id": "64scYLphagep"
      },
      "id": "64scYLphagep"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test) # Make predictions on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred) # Evaluate the model\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9xi1_UFdaZuW"
      },
      "id": "9xi1_UFdaZuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Fold Cross-Validation:\n",
        "\n",
        "K-Fold Cross-Validation is a powerful technique used to assess the performance of a machine learning model on unseen data. It helps to prevent overfitting and provides a more reliable estimate of model performance.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "1. Splitting the Data: The dataset is divided into K equal-sized folds.\n",
        "2. Training and Testing:\n",
        "  - For each fold:\n",
        "    - One fold is used as the testing set.\n",
        "    - The remaining K-1 folds are used as the training set.\n",
        "    - The model is trained on the training set and evaluated on the testing set.\n",
        "3. Averaging the Results: The performance metrics (e.g., accuracy, precision, recall, F1-score) from each fold are averaged to get a final estimate of the model's performance.\n",
        "\n",
        "[KFold](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.KFold.html)"
      ],
      "metadata": {
        "id": "JbYfGr4cUU_L"
      },
      "id": "JbYfGr4cUU_L"
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "Xn = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
        "kf = KFold(n_splits=2)\n",
        "\n",
        "print(kf)\n",
        "for i, (train_index, test_index) in enumerate(kf.split(Xn)):\n",
        "    print(f\"Fold {i}:\")\n",
        "    print(f\"  Train: index={train_index}\")\n",
        "    print(f\"  Test:  index={test_index}\")"
      ],
      "metadata": {
        "id": "vsbp_Fl4UVbt"
      },
      "id": "vsbp_Fl4UVbt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## APPLYING to DATA ########\n",
        "X = data[selected_features].values  # Use only selected features for training\n",
        "y = data[\"Survived\"].values\n",
        "# Create a KFold object with 5 folds\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression()\n",
        "# Perform K-Fold Cross-Validation\n",
        "scores = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    scores.append(score)\n",
        "\n",
        "print(\"Accuracy scores:\", scores)\n",
        "print(\"Average accuracy:\", sum(scores) / len(scores))"
      ],
      "metadata": {
        "id": "g8ZkpiFfWRQX"
      },
      "id": "g8ZkpiFfWRQX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified K-Fold Cross-Validation\n",
        "\n",
        "It is a technique used in machine learning to evaluate a model's performance on imbalanced datasets. It ensures that each fold contains a representative sample of each class, making the evaluation more reliable.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "1. Stratification: The dataset is divided into strata based on the target variable. This ensures that each stratum contains a similar distribution of classes.\n",
        "2. K-Fold Split: The data within each stratum is then split into K folds.\n",
        "3. Cross-Validation: The model is trained and evaluated on different combinations of folds, ensuring that each fold contains a representative sample of each class.\n",
        "\n",
        "[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)"
      ],
      "metadata": {
        "id": "rm9OZZ1RZOfe"
      },
      "id": "rm9OZZ1RZOfe"
    },
    {
      "cell_type": "code",
      "source": [
        "######## EXAMPLE #########\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "Xn1 = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
        "yn1 = np.array([0, 0, 1, 1])\n",
        "skf = StratifiedKFold(n_splits=2)\n",
        "print(skf)\n",
        "for i, (train_index, test_index) in enumerate(skf.split(Xn1, yn1)):\n",
        "    print(f\"Fold {i}:\")\n",
        "    print(f\"  Train: index={train_index}\")\n",
        "    print(f\"  Test:  index={test_index}\")"
      ],
      "metadata": {
        "id": "zTlbnhRnX13e"
      },
      "id": "zTlbnhRnX13e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## APPLYING to DATA ########\n",
        "X = data[selected_features].values  # Use only selected features for training\n",
        "y = data[\"Survived\"].values\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "print(skf)\n",
        "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    scores.append(score)\n",
        "\n",
        "print(\"Accuracy scores:\", scores)\n",
        "print(\"Average accuracy:\", sum(scores) / len(scores))"
      ],
      "metadata": {
        "id": "YWeyF3E4YJHP"
      },
      "id": "YWeyF3E4YJHP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supervised Learning Models\n",
        "\n",
        "Classification Models:\n",
        "\n",
        "- KNeighborsClassifier\n",
        "- Decision Tree Classifier\n",
        "- Gaussian Naive Bayes\n",
        "- Support Vector Machine (SVM)\n",
        "- Logistic Regression\n",
        "- Neural Networks\n",
        "  - Multi-layer Perceptron (MLP)\n",
        "- Ensemble Methods (Boosting, Random Forests)\n",
        "  - Random Forest Classifier\n",
        "  - Gradient Boosting Classifier\n",
        "\n",
        "Regression Models:\n",
        "\n",
        "- Linear Regression"
      ],
      "metadata": {
        "id": "uk3oIHDYcpyi"
      },
      "id": "uk3oIHDYcpyi"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "1DsjKYs7dFoq"
      },
      "id": "1DsjKYs7dFoq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}